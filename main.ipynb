{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67c6a863-5fd5-4825-8fc8-ccc5341747be",
   "metadata": {},
   "source": [
    "## Gradient Boosted Trees\n",
    "\n",
    "This is an implementation of Gradient Boosted Trees for continuous datasets\n",
    "\n",
    "First we look at importing the required datasets, then defining helper functions for developing a Regression Tree, then relevant classes defining the Regression Tree implementation. After this, the performance of the Regression Tree is compared with Sklearn's DecisionTreeRegressor, showing that for multiple min_samples_split both implementations show very close mse values with test data. \n",
    "\n",
    "Then we implement a Gradient Boosted Regression Tree with 5 Trees, and using Mango, a Python library used to find hyperparameters for expensive models, we look at some different implementations using Mango. The main regularisation technique used for our tree is the minimum number of samples split in each node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eeb107b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install numpy pandas scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d373aed9-3bad-4c80-ab98-94f4fab252f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([8], dtype='int64')\n",
      "(14448, 8) (6192, 8) (14448, 1) (6192, 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "data = fetch_california_housing(as_frame=True)\n",
    "df = data.frame\n",
    "df.columns = range(len(df.columns))\n",
    "X = df.iloc[:, :-1]\n",
    "y = df.iloc[:, -1]\n",
    "\n",
    "# Split into train/test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=999)\n",
    "y_train = pd.DataFrame(y_train, columns=[y.name])\n",
    "y_test = pd.DataFrame(y_test, columns=[y.name])\n",
    "\n",
    "\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "# X_train = pd.read_csv(\"train_x.csv\", header=None)\n",
    "# y_train = pd.read_csv(\"train_y.csv\", header=None)\n",
    "# X_test = pd.read_csv(\"test_x.csv\", header=None)\n",
    "# y_test = pd.read_csv(\"test_y.csv\", header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cace07-fcd2-4eeb-b59b-8217f64efe27",
   "metadata": {},
   "source": [
    "### Helper functions\n",
    "\n",
    "Here, we define helper functions for fitting the Regression Tree on some training dataset $X$ and $y$. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "068d2924-fd63-40f0-8e6d-c0bae6d63215",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The formula for mse can be rewritten, as the ground_truth is the mean of y values.\n",
    "# This function uses the sum of y values, the sum of squared y values, and the number of y values to update mse in O(1)\n",
    "def update_mse(y_sum, y_squared_sum, n):\n",
    "    return ((y_squared_sum/n) - (y_sum / n)**2)\n",
    "    # return ((y_squared_sum/n) - (mean)**2)\n",
    "\n",
    "# Function to find the best split given a dataset, in which the mse of both splits are minimum\n",
    "def best_mse_split(df):\n",
    "    n, features = df.iloc[:, :-1].shape\n",
    "    min_mse = np.inf\n",
    "    best_feature = 0\n",
    "    best_value = 0\n",
    "    best_left_split = []\n",
    "    best_right_split = []\n",
    "\n",
    "    # iterating over every feature (feature column in our dataframe)\n",
    "    for f in range(features):\n",
    "        # we sort the dataframe by the current feature -> O(n log n) where n is the number of examples\n",
    "        df_sorted = df.sort_values(by=f, inplace=False)\n",
    "        y = df_sorted.iloc[:,-1]\n",
    "        total_y_sum = y.sum()\n",
    "        total_y_squared_sum = (y**2).sum()\n",
    "        left_split_y_sum = 0\n",
    "        left_split_y_squared_sum = 0\n",
    "        # iterating over every feature vector/ example in the dataset\n",
    "        for i in range(n-1):  \n",
    "            # this is O(1), instead of recalculating the mse for the dataset, we can incrementally add an example to the left split and remove it from the right\n",
    "            # Since calculating mse with the traditional formula takes O(n), by sorting the current feature, we can incrementally control the size of each split\n",
    "            # and calculate mse of each split in constant time\n",
    "            left_split_y_sum += y.iloc[i]\n",
    "            left_split_y_squared_sum += (y.iloc[i])**2\n",
    "            right_split_y_sum = total_y_sum - left_split_y_sum\n",
    "            right_split_y_squared_sum = total_y_squared_sum - left_split_y_squared_sum\n",
    "            left_n = i + 1\n",
    "            right_n = n - left_n\n",
    "            \n",
    "            # we skip duplicate values, if our current dataframe only contains duplicate feature vectors, the algorithm will try to split the dataframe into two, which should be avoided\n",
    "            if df_sorted.iloc[i,f] == df_sorted.iloc[i+1,f]:\n",
    "                continue\n",
    "\n",
    "            left_mse = update_mse(left_split_y_sum, left_split_y_squared_sum, left_n)\n",
    "            right_mse = update_mse(right_split_y_sum, right_split_y_squared_sum, right_n)\n",
    "            split_mse = ((left_n*left_mse)+(right_n*right_mse))/n\n",
    "\n",
    "            # finding current best split\n",
    "            if split_mse < min_mse:\n",
    "                min_mse = split_mse\n",
    "                best_feature = f\n",
    "                best_value = df_sorted.iloc[i, f]\n",
    "                # finds the indices where the left split and right split are in the original unsorted dataframe respectively\n",
    "                best_left_split = np.where(df.iloc[:, f] <= best_value)[0]\n",
    "                best_right_split = np.where(df.iloc[:, f] > best_value)[0]\n",
    "    \n",
    "    return min_mse, best_feature, best_value, best_left_split, best_right_split\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "1b7caa93-3695-4324-8d6f-caa4f3636654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recursive function used to build the tree \n",
    "def build(df, min_samples):\n",
    "    n = df.shape[0]\n",
    "    # stopping condition -> min_samples\n",
    "    # if the size of the current dataframe in this call is too small we return the mean of the ground truth values\n",
    "    # becomes a leaf node in our tree\n",
    "    if n < min_samples:\n",
    "        return np.mean(df.iloc[:,-1])\n",
    "\n",
    "    # find the best mse split for the current dataset\n",
    "    mse, feature, value, left_split, right_split = best_mse_split(df)\n",
    "    \n",
    "    # no split could be found, make it a leaf node\n",
    "    # no non-empty split could be found -> remember if we had duplicate feature vectors for our entire dataset, no split could be found\n",
    "    # as the mse is never updated, so we return the mean of ground truth values\n",
    "    if mse == np.inf:\n",
    "        return np.mean(df.iloc[:,-1])\n",
    "\n",
    "    # recursively build left and right child of current node\n",
    "    # using the left and right split respectively\n",
    "    left_child = build(df.iloc[left_split], min_samples)\n",
    "    right_child = build(df.iloc[right_split], min_samples)\n",
    "\n",
    "    return TreeNode(mse, feature, value, left_child, right_child)\n",
    "\n",
    "# helper function to navigate tree when predicting a new feature vector\n",
    "def navigate_tree(tree, unseen):\n",
    "    # leaf node\n",
    "    if not isinstance(tree,TreeNode):\n",
    "        return tree\n",
    "\n",
    "    feature = tree.feature\n",
    "    value = tree.value\n",
    "\n",
    "    # recursively call navigate tree\n",
    "    if unseen[feature] <= value:\n",
    "        return navigate_tree(tree.left, unseen)\n",
    "    return navigate_tree(tree.right, unseen)\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1363e10-d33d-4a51-8c88-42c9a9af1889",
   "metadata": {},
   "source": [
    "### Regression Tree\n",
    "\n",
    "Below we define the classes for a Tree and the custom RegressionTree class, in which we then compare the performance between the RegressionTree class and Scikit's DecisionTreeRegressor, using mse as the main metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "9d63ba00-5716-4dd2-9238-463856dfa55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining custom Tree class, where each node stores the mse and pointers to children\n",
    "class TreeNode:\n",
    "\n",
    "    def __init__(self, mse, feature, value, left, right):\n",
    "        self.mse = mse\n",
    "        self.feature = feature\n",
    "        self.value = value\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "\n",
    "# RegressionTree class, our main Regression Tree implementation\n",
    "class RegressionTree:\n",
    "\n",
    "    # storing hyperparameter \n",
    "    def __init__(self, min_samples_split):\n",
    "        self.tree = None\n",
    "        self.min_samples_split = min_samples_split\n",
    "        \n",
    "\n",
    "    # we must concatenate X and y into one dataset for the build function, so we do not have to do that everytime we want to calculate the best mse split\n",
    "    def fit_tree(self, X, y):\n",
    "        y = y.rename(columns={0: X.shape[1]})\n",
    "        train = pd.concat([X,y], axis=1)\n",
    "        # call recursive build function to create Regression Tree\n",
    "        self.tree = build(train, self.min_samples_split)\n",
    "\n",
    "    # predicting y_values for each feature vector in X_test\n",
    "    def predict(self, X):\n",
    "        y = []\n",
    "        for c, r in X.iterrows():\n",
    "            y.append(navigate_tree(self.tree, r))\n",
    "        return np.array(y)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "bcbd8ee9-bb7a-44db-9f13-61d2e3117f75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39.498220682144165\n"
     ]
    }
   ],
   "source": [
    "# example usage \n",
    "import time\n",
    "time_start = time.time()\n",
    "rt = RegressionTree(100)\n",
    "rt.fit_tree(X_train, y_train)\n",
    "time_end = time.time()\n",
    "print(time_end-time_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "abc1c73d-0a7e-48e4-8648-fdb2d703ebe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My Regression Tree [0.3770890818150133, 0.36488124820066653, 0.367726984309839, 0.386957383765939, 0.403014212850536, 0.41172716753812144, 0.4164127326689568, 0.4328280747548593, 0.4457439165854728, 0.4533684941844094]\n",
      "Sklearns regression tree [0.36915327689111344, 0.3608922430135572, 0.36442710197187267, 0.38450484025752985, 0.40136222742026934, 0.4100466429725798, 0.4146757834851642, 0.4315985411513649, 0.44544823790228105, 0.4528911646969541]\n",
      "\n",
      "MSE difference between sklearn's Decision Regressor and my Regression Tree: 1.0619058862773123e-05\n"
     ]
    }
   ],
   "source": [
    "# Running time -> a few minutes\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# We create a RegressionTree and DecisionTreeRegressor, using a variety of min_samples_split values (the only hyperparameter for the RegressionTree)\n",
    "# we then fit the trees on X_train and y_train and allow them to make predictions on X_test\n",
    "# we calculate the mse between predictions and y_test, and append them to an array.\n",
    "rt_mse = []\n",
    "scikit_rt_mse = []\n",
    "for i in range(50,501,50):\n",
    "    rt = RegressionTree(i)\n",
    "    rt.fit_tree(X_train, y_train)\n",
    "\n",
    "    scikit_rt = DecisionTreeRegressor(criterion='squared_error', min_samples_split = i)\n",
    "    scikit_rt.fit(X_train, y_train)\n",
    "\n",
    "    rt_prediction = rt.predict(X_test)\n",
    "    rt_mse.append(mean_squared_error(y_test, rt_prediction))\n",
    "    scikit_prediction = scikit_rt.predict(X_test)\n",
    "    scikit_rt_mse.append(mean_squared_error(y_test, scikit_prediction))\n",
    "\n",
    "# Just by printing the arrays, we see the results are very similiar for min_samples_split ranging from 50 to 500\n",
    "print(f\"My Regression Tree {rt_mse}\")\n",
    "print(f\"Sklearns regression tree {scikit_rt_mse}\")\n",
    "# we can go a step further and calculate mse between the two arrays, as mse is just calculating difference in results\n",
    "final_mse = mean_squared_error(rt_mse, scikit_rt_mse)\n",
    "print(\"\")\n",
    "print(f\"MSE difference between sklearn's Decision Regressor and my Regression Tree: {final_mse}\")\n",
    "\n",
    "# extremely small, meaning my implementation is likely correct\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665477d0-bb50-40cd-9f3a-2bcf20a45e28",
   "metadata": {},
   "source": [
    "### Ensemble of Regression Trees\n",
    "\n",
    "Next we will look at an Ensemble of Regression Trees implementation which uses the RegressionTree class:\n",
    "\n",
    "To fit the tree, we initially set the residuals to just be the target values $y$ (of the training set). We call a RegressionTree with min_samples_split[$i$], where min_samples_split is an array in which each index represents the min_samples_split hyperparameter of each tree $T_i$.\n",
    "\n",
    "We fit the first tree $T_0$ with $X$_train and the residuals which are just the target values $y$, and then modify the total predictions (currently an array of 0's)s uch that we add the predictions of $T_0$ on $y$. In subsequent iterations, $T_i$ is trained with the residuals, which is the difference between the target values $y$ and the current total predictions $y_pred$, and we of course append the constructed trees to an array.\n",
    "\n",
    "When we want to predict a $y$-value, we just feed each $T_i$ the test data $X$, and add those predictions to a final value $y$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "146d5f19-226a-4afb-b78b-03ab57d41db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RegressionTreeEnsemble class\n",
    "class RegressionTreeEnsemble:\n",
    "\n",
    "    # We store n -> number of trees to be implemented and min_samples_split, an array representing the min_samples_split values for n trees\n",
    "    def __init__(self, n, min_samples_split):\n",
    "        self.n = n\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.trees = []\n",
    "\n",
    "    # fit function explained above\n",
    "    def fit_tree(self, X, y):\n",
    "        if not self.n <= 0:\n",
    "            y_pred = pd.DataFrame(0, index=y.index, columns=y.columns)\n",
    "            for i in range(self.n):\n",
    "                residuals = y - y_pred\n",
    "                tree_n = RegressionTree(self.min_samples_split[i])\n",
    "                tree_n.fit_tree(X, residuals)\n",
    "                y_pred += pd.DataFrame(tree_n.predict(X), index=y.index, columns = y.columns)         \n",
    "                self.trees.append(tree_n)\n",
    "\n",
    "    # predict function explained above\n",
    "    def predict(self, X):\n",
    "        if not self.trees:\n",
    "            print(\"test\")\n",
    "            return np.array([])\n",
    "\n",
    "        # first declare y as a matrix of 0's, since we add our predictions onto it from each tree\n",
    "        y = np.full(X.shape[0], 0, dtype=np.float64)\n",
    "        print(y)\n",
    "        for tree in self.trees:\n",
    "            print(tree.predict(X))\n",
    "            y += tree.predict(X)\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "47db9ec7-ce1b-46b2-9e26-bc24171f1c7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0.98978212 1.55701765 1.72766144 ... 2.03436601 1.00918155 0.98978212]\n",
      "[-0.3654388   0.32357427  0.41958241 ... -0.27450879  0.09375542\n",
      " -0.3654388 ]\n",
      "[ 0.06946423  0.37233534  0.11039601 ... -0.08507303 -0.14142553\n",
      "  0.03686439]\n",
      "[ 0.1882578  -0.11478428 -0.35998773 ...  0.14316987 -0.07072568\n",
      "  0.1882578 ]\n",
      "[ 0.12156484 -0.19243007 -0.57086568 ...  0.18968323 -0.0369005\n",
      "  0.12156484]\n",
      "MSE produced by an Ensemble Regression Tree for min_samples_split = [500, 400, 300, 200, 100]: 0.34227737631410904\n"
     ]
    }
   ],
   "source": [
    "# example usage -> takes a few minutes\n",
    "min_samples = [500, 400, 300, 200, 100]\n",
    "rte = RegressionTreeEnsemble(5, min_samples)\n",
    "rte.fit_tree(X_train, y_train)\n",
    "rte_predictions = rte.predict(X_test)\n",
    "# print(rte_predictions)\n",
    "print(f\"MSE produced by an Ensemble Regression Tree for min_samples_split = {min_samples}: {mean_squared_error(y_test, rte_predictions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63cde34b-7bf7-4242-86f2-7e9620bf7b09",
   "metadata": {},
   "source": [
    "### Hyperparameter Fine-tuning\n",
    "\n",
    "To achieve the best performance out of the Ensemble Regression Tree on our dataset, fine-tuning of the hyperparameter min_samples_split is required. For the RegressionTree implementation, it may be possible to iterate over a range of min_samples_split value and take the one that yields the lowest MSE, but due to how expensive it is to fit an EnsembleRegressionTree (taking a few minutes each) and the fact that each tree can have different min_samples_split values, careful fine-tuning is required to find a good performing set of min_samples_split parameters whilst also not taking days or weeks to find them. To do this, some hyperparameter fine tuning libraries were used:\n",
    "\n",
    "#### Skopt\n",
    "\n",
    "Also known as Scikit Optimize, it has a function gp_minimize, which takes an objective function (which we define in our code) and a set of potential parameters, which for the Ensemble Regression Tree is a range from 2 to 500 for each tree in the Ensemble. It uses Bayesian Optimisation based on a Gaussian Process Model, which first samples the hyperparameter space by evaluating the objective function at random points, and then fits a probabilistic model to predict the objective function in the hyperparameter space. Iteratively, it selects a point of interest and evaluates the objective function at that point, and updates its model to predict where the global minima of the function is. \n",
    "\n",
    "This is used with initially 30 calls to the objective function, which took around 4000 seconds to complete, and did not show promising results.\n",
    "\n",
    "#### Mango\n",
    "\n",
    "Mango is a parallel hyperparameter tuning library that is capable of fine-tuning in parallel and also uses search techniques based on Bayesian Optimisation. This library is in use by ARM with its main feature being parallelisation. \n",
    "\n",
    "This library gave much more promising results than Skopt, however running in parallel does take longer, as we do not split the workload between cores, we let each thread find a minima in parallel, so the number of calls to the objective function is the same for each core as it is for one core.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "4c787771-a48e-4688-bce4-80877e49e0c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting arm-mango\n",
      "  Using cached arm_mango-1.4.3-py3-none-any.whl (29 kB)\n",
      "Collecting scikit-optimize\n",
      "  Downloading scikit_optimize-0.10.2-py2.py3-none-any.whl (107 kB)\n",
      "\u001b[K     |████████████████████████████████| 107 kB 6.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting attrdict>=2.0.1\n",
      "  Downloading attrdict-2.0.1-py2.py3-none-any.whl (9.9 kB)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /Users/oliverbrown/Library/Python/3.9/lib/python/site-packages (from arm-mango) (2.0.2)\n",
      "Requirement already satisfied: scikit_learn>=0.21.3 in /Users/oliverbrown/Library/Python/3.9/lib/python/site-packages (from arm-mango) (1.6.1)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /Users/oliverbrown/Library/Python/3.9/lib/python/site-packages (from arm-mango) (1.13.1)\n",
      "Collecting tqdm>=4.36.1\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Collecting pyaml>=16.9\n",
      "  Downloading pyaml-25.5.0-py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: packaging>=21.3 in /Users/oliverbrown/Library/Python/3.9/lib/python/site-packages (from scikit-optimize) (25.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /Users/oliverbrown/Library/Python/3.9/lib/python/site-packages (from scikit-optimize) (1.5.1)\n",
      "Requirement already satisfied: six in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from attrdict>=2.0.1->arm-mango) (1.15.0)\n",
      "Collecting PyYAML\n",
      "  Downloading PyYAML-6.0.2-cp39-cp39-macosx_11_0_arm64.whl (172 kB)\n",
      "\u001b[K     |████████████████████████████████| 172 kB 11.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: threadpoolctl>=3.1.0 in /Users/oliverbrown/Library/Python/3.9/lib/python/site-packages (from scikit_learn>=0.21.3->arm-mango) (3.6.0)\n",
      "Installing collected packages: PyYAML, tqdm, pyaml, attrdict, scikit-optimize, arm-mango\n",
      "Successfully installed PyYAML-6.0.2 arm-mango-1.4.3 attrdict-2.0.1 pyaml-25.5.0 scikit-optimize-0.10.2 tqdm-4.67.1\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.1.1 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install arm-mango scikit-optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "36d39888-a425-4d49-8d8f-8dd743fe6e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple objective function to be optimised\n",
    "def obj_func_skopt(min_samples_split):\n",
    "    # uncomment line below if does not work\n",
    "    # min_samples_split = [p0, p1, p2, p3, p4]\n",
    "    model = RegressionTreeEnsemble(5, min_samples_split)\n",
    "    model.fit_tree(X_train, y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "\n",
    "    mse = mean_squared_error(y_true = y_test, y_pred = predictions)\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a827427c-a8ae-4003-8982-4d56346d0b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0.97636364 1.71463158 2.58634286 ... 1.75236905 1.18534848 0.97636364]\n",
      "[ 0.00319529  0.11207886  0.19603182 ...  0.15026209 -0.07134906\n",
      "  0.08224575]\n",
      "[ 0.22303895 -0.26485936  0.01530204 ...  0.01687752  0.05273895\n",
      " -0.00360871]\n",
      "[ 0.0500158  -0.10737012 -0.07253413 ...  0.00027411  0.00577622\n",
      "  0.0190893 ]\n",
      "[-0.01213502  0.37074462  0.2513331  ... -0.09823516 -0.15988213\n",
      " -0.01213502]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0.98978212 1.55701765 2.58634286 ... 1.86397585 1.00918155 0.98978212]\n",
      "[-0.06635228  0.37629595  0.0550859  ...  0.06758971  0.04894126\n",
      " -0.06635228]\n",
      "[ 0.07110089  0.28668838 -0.10511274 ...  0.14868546  0.0603098\n",
      "  0.07110089]\n",
      "[-0.00394595 -0.20914449  0.1265228  ...  0.00960049 -0.08302613\n",
      " -0.03768355]\n",
      "[-0.03460922  0.08198381  0.02953271 ...  0.1136396   0.03067969\n",
      "  0.17797456]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0.90818382 1.71463158 2.58634286 ... 1.75236905 1.18534848 0.90818382]\n",
      "[-0.38877119 -0.08383341  0.20505888 ...  0.06014207 -0.06998391\n",
      " -0.38877119]\n",
      "[ 0.09526997  0.03978228 -0.09613626 ... -0.03381231 -0.09352573\n",
      "  0.09526997]\n",
      "[ 0.10342254 -0.01158592  0.07756161 ... -0.16310727  0.10861175\n",
      " -0.11528603]\n",
      "[ 0.05083215 -0.13038649  0.2517639  ... -0.0438903   0.05737753\n",
      "  0.05083215]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0.98978212 1.55701765 1.72766144 ... 2.03436601 1.00918155 0.98978212]\n",
      "[-0.29211524  0.30342622  0.41004679 ... -0.28695106  0.08606622\n",
      " -0.29211524]\n",
      "[ 0.12064313  0.03930225  0.0727679  ... -0.12905928 -0.08789292\n",
      "  0.1638241 ]\n",
      "[-0.03864698 -0.15210551  0.0036708  ...  0.13846384  0.06228236\n",
      " -0.03864698]\n",
      "[ 0.05172487 -0.10117822  0.17782535 ...  0.08459111 -0.01631989\n",
      "  0.12929237]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0.98978212 1.55701765 1.72766144 ... 2.03436601 1.00918155 0.98978212]\n",
      "[-0.27706561  0.45195943  0.41325661 ...  0.0300474  -0.1000417\n",
      " -0.27706561]\n",
      "[ 0.11103735  0.28357407  0.38680847 ...  0.02767499 -0.22701067\n",
      "  0.11103735]\n",
      "[-0.06394805 -0.1687851   0.23012752 ...  0.0495454   0.15622131\n",
      " -0.07462281]\n",
      "[ 0.07050406 -0.16851869 -0.1509645  ... -0.03318212  0.07026106\n",
      " -0.08502088]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0.98978212 1.55701765 2.58634286 ... 1.75236905 1.18534848 0.98978212]\n",
      "[-0.110565   -0.06934736  0.48478229 ... -0.07265105 -0.03587188\n",
      " -0.110565  ]\n",
      "[ 0.08295739 -0.1803164  -0.01984101 ... -0.01462172 -0.0961128\n",
      "  0.08295739]\n",
      "[ 0.00923853  0.41704813 -0.03579765 ...  0.03404919  0.22080419\n",
      "  0.12369258]\n",
      "[-0.02426686 -0.0781152  -0.14623088 ... -0.02845171 -0.19292206\n",
      " -0.08839667]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[1.27242857 2.11323333 3.17782609 ... 1.83387879 1.00930769 0.86240625]\n",
      "[-0.13177086  0.10554282 -0.12027436 ...  0.04197735 -0.06630135\n",
      " -0.13177086]\n",
      "[-0.24887653 -0.03824144  0.14100352 ...  0.16585601 -0.10356029\n",
      " -0.01053636]\n",
      "[ 0.03851847  0.5034807   0.14289214 ... -0.17782161 -0.02898037\n",
      " -0.31014736]\n",
      "[ 0.01461789  0.02550715  0.04634467 ...  0.0378804  -0.01519474\n",
      "  0.02550715]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0.98978212 1.55701765 2.58634286 ... 1.98638122 1.00918155 0.98978212]\n",
      "[-0.27466213  0.36571102  0.27334379 ...  0.01972029 -0.19161073\n",
      " -0.27466213]\n",
      "[ 0.10252028  0.16081986 -0.43502459 ... -0.08153957 -0.09342049\n",
      "  0.16081986]\n",
      "[ 0.12761147 -0.10351657  0.20062553 ...  0.05466827 -0.05433763\n",
      " -0.04097142]\n",
      "[-0.04016584 -0.02447856  0.14770427 ... -0.03422728 -0.1915041\n",
      "  0.18749728]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0.98978212 1.55701765 2.58634286 ... 1.86397585 1.18534848 0.98978212]\n",
      "[-0.16183681  0.38092939 -0.12682035 ...  0.07036115 -0.05572089\n",
      "  0.05049565]\n",
      "[-0.11367575 -0.12515632  0.13142782 ... -0.02591916 -0.01331485\n",
      " -0.11367575]\n",
      "[ 0.00677692 -0.07939787 -0.10142411 ...  0.00587671  0.10160712\n",
      "  0.00677692]\n",
      "[ 0.00861084 -0.11855539  0.03057923 ...  0.04073918 -0.01639619\n",
      "  0.081556  ]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[1.0574     2.11323333 3.17782609 ... 1.83387879 1.00930769 0.86240625]\n",
      "[-0.15063557  0.28037024 -0.00157547 ... -0.08165436 -0.13332366\n",
      " -0.03802957]\n",
      "[ 0.01742284 -0.04019529  0.09212121 ... -0.00183059  0.13692\n",
      "  0.01742284]\n",
      "[ 0.14349318 -0.28002548  0.05912486 ...  0.21135816  0.26995041\n",
      "  0.1492634 ]\n",
      "[ 0.01803336 -0.02067543 -0.05133309 ...  0.06525208 -0.06936312\n",
      "  0.01803336]\n",
      "          fun: 0.3303020837527442\n",
      "            x: [np.int64(467), np.int64(312), np.int64(377), np.int64(445), np.int64(152)]\n",
      "    func_vals: [ 3.454e-01  4.079e-01  4.149e-01  3.303e-01  3.568e-01\n",
      "                 3.825e-01  4.721e-01  3.703e-01  3.770e-01  4.380e-01]\n",
      "      x_iters: [[np.int64(136), np.int64(183), np.int64(347), np.int64(299), np.int64(175)], [np.int64(344), np.int64(184), np.int64(251), np.int64(395), np.int64(20)], [np.int64(142), np.int64(99), np.int64(286), np.int64(25), np.int64(366)], [np.int64(467), np.int64(312), np.int64(377), np.int64(445), np.int64(152)], [np.int64(404), np.int64(447), np.int64(157), np.int64(427), np.int64(133)], [np.int64(189), np.int64(418), np.int64(172), np.int64(124), np.int64(137)], [np.int64(44), np.int64(298), np.int64(39), np.int64(8), np.int64(362)], [np.int64(372), np.int64(222), np.int64(462), np.int64(385), np.int64(73)], [np.int64(328), np.int64(35), np.int64(232), np.int64(291), np.int64(137)], [np.int64(61), np.int64(480), np.int64(155), np.int64(15), np.int64(347)]]\n",
      "       models: [GaussianProcessRegressor(kernel=1**2 * Matern(length_scale=[1, 1, 1, 1, 1], nu=2.5) + WhiteKernel(noise_level=1),\n",
      "                                        n_restarts_optimizer=2, noise='gaussian',\n",
      "                                        normalize_y=True, random_state=1992265279)]\n",
      "        space: Space([Integer(low=2, high=500, prior='uniform', transform='normalize'),\n",
      "                      Integer(low=2, high=500, prior='uniform', transform='normalize'),\n",
      "                      Integer(low=2, high=500, prior='uniform', transform='normalize'),\n",
      "                      Integer(low=2, high=500, prior='uniform', transform='normalize'),\n",
      "                      Integer(low=2, high=500, prior='uniform', transform='normalize')])\n",
      " random_state: RandomState(MT19937)\n",
      "        specs:     args:                    func: <function obj_func_skopt at 0x14ea67430>\n",
      "                                      dimensions: Space([Integer(low=2, high=500, prior='uniform', transform='normalize'),\n",
      "                                                         Integer(low=2, high=500, prior='uniform', transform='normalize'),\n",
      "                                                         Integer(low=2, high=500, prior='uniform', transform='normalize'),\n",
      "                                                         Integer(low=2, high=500, prior='uniform', transform='normalize'),\n",
      "                                                         Integer(low=2, high=500, prior='uniform', transform='normalize')])\n",
      "                                  base_estimator: GaussianProcessRegressor(kernel=1**2 * Matern(length_scale=[1, 1, 1, 1, 1], nu=2.5),\n",
      "                                                                           n_restarts_optimizer=2, noise='gaussian',\n",
      "                                                                           normalize_y=True, random_state=1992265279)\n",
      "                                         n_calls: 10\n",
      "                                 n_random_starts: None\n",
      "                                n_initial_points: 10\n",
      "                         initial_point_generator: random\n",
      "                                        acq_func: gp_hedge\n",
      "                                   acq_optimizer: auto\n",
      "                                              x0: None\n",
      "                                              y0: None\n",
      "                                    random_state: RandomState(MT19937)\n",
      "                                         verbose: False\n",
      "                                        callback: None\n",
      "                                        n_points: 10000\n",
      "                            n_restarts_optimizer: 5\n",
      "                                              xi: 0.01\n",
      "                                           kappa: 1.96\n",
      "                                          n_jobs: 1\n",
      "                                model_queue_size: None\n",
      "                                space_constraint: None\n",
      "               function: base_minimize\n",
      "Time taken 3175.846538066864\n"
     ]
    }
   ],
   "source": [
    "# Skopt implementation\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Integer\n",
    "\n",
    "# this took about an hour\n",
    "search_space = list()\n",
    "for i in range(5):\n",
    "    search_space.append(Integer(2, 500, name=f\"p{i}\"))\n",
    "time_start = time.time()\n",
    "result = gp_minimize(obj_func_skopt, search_space, n_calls=10)\n",
    "print(result)\n",
    "time_end = time.time()\n",
    "print(f\"Time taken {time_end-time_start}\")\n",
    "\n",
    "# ok results\n",
    "# fun: 0.3303020837527442\n",
    "# x: [np.int64(467), np.int64(312), np.int64(377), np.int64(445), np.int64(152)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39c6795-4dc4-42ca-aff1-151dafe41f52",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mango'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Mango implementation \u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmango\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tuner, scheduler\n\u001b[1;32m      4\u001b[0m param_space \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m()\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mango'"
     ]
    }
   ],
   "source": [
    "# Mango implementation \n",
    "from mango import Tuner, scheduler\n",
    "\n",
    "param_space = dict()\n",
    "for i in range(5):\n",
    "    param_space[f'p{i}'] = range(2,500)\n",
    "\n",
    "# Using 4 threads/cores and running in parallel -> 80 iterations total technically\n",
    "config = {\n",
    "    'num_iteration': 20,  \n",
    "    'batch_size': 4     \n",
    "}\n",
    "\n",
    "\n",
    "@scheduler.parallel(n_jobs=4)\n",
    "def obj_func_mango(**params):\n",
    "    min_samples_split = [params['p0'],params['p1'],params['p2'],params['p3'],params['p4']]\n",
    "    model = RegressionTreeEnsemble(5, min_samples_split)\n",
    "    model.fit_tree(X_train, y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "\n",
    "    mse = mean_squared_error(y_true = y_test, y_pred = predictions)\n",
    "    return mse\n",
    "\n",
    "# Took about 0.75 hours (less iterations than skopt)\n",
    "tuner = Tuner(param_space, obj_func_mango, config)\n",
    "time_start = time.time()\n",
    "results = tuner.minimize()  # Run Tuner\n",
    "best_params = results[\"best_params\"]\n",
    "best_objective = results[\"best_objective\"]\n",
    "print(f'Optimal value of parameters: {best_params} and objective: {best_objective}')\n",
    "time_end = time.time()\n",
    "print(f\"Time taken {time_end-time_start}\")\n",
    "\n",
    "\n",
    "# Best score: 0.5036546753541488: 100%|█████████████████████████████████████████████| 20/20 [46:45<00:00, 140.30s/it]\n",
    "# Optimal value of parameters: {'p0': 275, 'p1': 371, 'p2': 240, 'p3': 400, 'p4': 333} and objective: 0.5036546753541488\n",
    "# Found betrter set of parameters\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ba1494-c660-4506-b488-07de064c41b2",
   "metadata": {},
   "source": [
    "### Another Approach\n",
    "\n",
    "To try get below 0.5 mse, we opted for finding the hyperparameters for each tree in the Ensemble one at a time instead of all at once. We alter our objective function to instead fit an Ensemble Tree with 5 trees, to fit an Ensemble with len(min_samples_split). And then we tell our optimiser to optimise on 1 tree first, finding the optimal min_samples_split $m_0$ for $T_0$, and then we tell our optimiser to optimise on 2 trees, where $m_0$ is already given and the optimiser must find $m_1$, so at each $T_i$ we are trying to find $m_i$ only and we are given all $m_0$ to $m_{i-1}$ already.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fa82d8-2d1c-4b39-be07-2b48442716a3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'scheduler' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# new objective function\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;129m@scheduler\u001b[39m\u001b[38;5;241m.\u001b[39mparallel(n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mobj_func\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams):\n\u001b[1;32m      4\u001b[0m     min_samples_split \u001b[38;5;241m=\u001b[39m  []\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(params)):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'scheduler' is not defined"
     ]
    }
   ],
   "source": [
    "# new objective function\n",
    "from mango import Tuner, scheduler\n",
    "\n",
    "@scheduler.parallel(n_jobs=4)\n",
    "def obj_func_mango_two(**params):\n",
    "    min_samples_split =  []\n",
    "    for i in range(len(params)):\n",
    "        min_samples_split.append(params[f'p{i}'])\n",
    "    model = RegressionTreeEnsemble(len(min_samples_split), min_samples_split)\n",
    "    model.fit_tree(X_train, y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "\n",
    "    mse = mean_squared_error(y_true = y_test, y_pred = predictions)\n",
    "    return mse\n",
    "\n",
    "current_optimal_params = []\n",
    "optimal_params_mse = []\n",
    "\n",
    "time_start = time.time()\n",
    "# iterating over each tree we want to find min_samples_split value for\n",
    "for n in range(1,6):\n",
    "    print(f\"finding minimizer for tree {n}\")\n",
    "    print(current_optimal_params)\n",
    "    print(optimal_params_mse)\n",
    "    param_space = dict()\n",
    "    # define the parameter space with values ranging from 2 to 500\n",
    "    for i in range(n):\n",
    "        param_space[f'p{i}'] = range(2,500)\n",
    "    # then, if we have already found optimal parameters for some trees we replace them in the parameter space with just a singular value\n",
    "    for i in range(len(current_optimal_params)):\n",
    "        param_space[f'p{i}'] = range(current_optimal_params[i], current_optimal_params[i]+1)\n",
    "\n",
    "    # This time 4 cores run in parallel and 20 calls to the objective function are made\n",
    "    config = {\n",
    "        'num_iteration': 20,  \n",
    "        'batch_size': 4 \n",
    "    }\n",
    "\n",
    "    tuner = Tuner(param_space, obj_func_mango_two, config)\n",
    "    results = tuner.minimize()  # Run Tuner\n",
    "    # we store the best mss in an array, and the mse for each mss\n",
    "    best_params = results[\"best_params\"]\n",
    "    current_optimal_params.append(best_params[f\"p{n-1}\"])\n",
    "    optimal_params_mse.append(results[\"best_objective\"])\n",
    "\n",
    "\n",
    "print(f\"Optimal min_samples_split array: {current_optimal_params}\")\n",
    "print(f\"The best mse for each tree: {optimal_params_mse}\")\n",
    "time_end = time.time()\n",
    "print(f\"Time taken {time_end-time_start}\")\n",
    "\n",
    "# Best score: 0.4963636646680849: 100%|██████████████████████████████████████████████| 20/20 [25:27<00:00, 76.39s/it]\n",
    "# [448, 464, 314, 374, 411]\n",
    "# [np.float64(0.5262064484548055), np.float64(0.5124900174963549), np.float64(0.5123641383276403), np.float64(0.49426267905059706), np.float64(0.4963636646680849)]\n",
    "# 4666.020900964737\n",
    "# num_iteration 20, batch_size 4, n_jobs 4\n",
    "# Took about 1-1.5 hours, we found a set of parameters yielding an mse of under 0.5\n",
    "# We increased the iterations to 50, batch_size to 5, still got the same result just took longer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9ca445-9b84-4cf3-83ae-1f0e0f867490",
   "metadata": {},
   "source": [
    "### K-Fold Cross Validation\n",
    "\n",
    "Next, to try get an even lower MSE, we implemented K-Fold Cross Validation on our training set $X_train$ in the objective function. This would multiply the time it takes to find a set of optimal parameters by 5 using $K=5$, but would help reduce overfitting to training data as we would train and test on a validation set using 5 different folds on the training set, then taking the average mse values of those folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc57c27-cdb8-4870-96ba-ca59ffa92c69",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'scheduler' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mpredict(X)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# New redefined objective function\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;129m@scheduler\u001b[39m\u001b[38;5;241m.\u001b[39mparallel(n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mobj_func\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams):\n\u001b[1;32m     22\u001b[0m     min_samples_split \u001b[38;5;241m=\u001b[39m  []\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(params)):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'scheduler' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "\n",
    "# Since our Ensemble Tree model is not an \"Estimator\" as required by the cross_val_score function from sklearn\n",
    "# we can easily convert this by creating a class inheriting from BaseEstimator and RegressorMixin\n",
    "class EnsembleTreeEstimator(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, min_samples_split):\n",
    "        self.min_samples_split = min_samples_split\n",
    "   \n",
    "    def fit(self, X, y):\n",
    "        self.model = RegressionTreeEnsemble(len(self.min_samples_split), self.min_samples_split)\n",
    "        self.model.fit_tree(X,y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)\n",
    "        \n",
    "# New redefined objective function\n",
    "@scheduler.parallel(n_jobs=4)\n",
    "def obj_func_mango_cv(**params):\n",
    "    min_samples_split =  []\n",
    "    for i in range(len(params)):\n",
    "        min_samples_split.append(params[f'p{i}'])\n",
    "    model = EnsembleTreeEstimator(min_samples_split=min_samples_split)\n",
    "    # Using Sklearn's KFold to automatically train and test on 5 different folds of the training set of data\n",
    "    # using mse still as our metric\n",
    "    kf = KFold(n_splits=5, shuffle=True)\n",
    "    mse_scores = cross_val_score(model, X_train, y_train, cv=kf, scoring=\"neg_mean_squared_error\")\n",
    "    mse_scores = -mse_scores\n",
    "    return mse_scores.mean()\n",
    "\n",
    "# Following the same process as before, extending our param space range from 2 to 600 instead of 2 to 500\n",
    "current_optimal_params = []\n",
    "optimal_params_mse = []\n",
    "\n",
    "time_start = time.time()\n",
    "for n in range(1,6):\n",
    "    print(f\"finding minimizer for tree {n}\")\n",
    "    print(current_optimal_params)\n",
    "    print(optimal_params_mse)\n",
    "    param_space = dict()\n",
    "    for i in range(n):\n",
    "        param_space[f'p{i}'] = range(2,600)\n",
    "    for i in range(len(current_optimal_params)):\n",
    "        param_space[f'p{i}'] = range(current_optimal_params[i], current_optimal_params[i]+1)\n",
    "        print(param_space[f'p{i}'])\n",
    "\n",
    "    config = {\n",
    "        'num_iteration': 20,  \n",
    "        'batch_size': 4   \n",
    "    }\n",
    "\n",
    "    tuner = Tuner(param_space, obj_func_mango_cv, config)\n",
    "    results = tuner.minimize()\n",
    "    best_params = results[\"best_params\"]\n",
    "    current_optimal_params.append(best_params[f\"p{n-1}\"])\n",
    "    optimal_params_mse.append(results[\"best_objective\"])\n",
    "\n",
    "print(f\"Optimal min_samples_split array: {current_optimal_params}\")\n",
    "print(f\"The best mse for each tree: {optimal_params_mse}\")\n",
    "time_end = time.time()\n",
    "print(f\"Time taken {time_end-time_start}\")\n",
    "\n",
    "# This yielded poor results, similar to the skopt optimiser\n",
    "# Best score: 0.5469522986173644: 100%|███████████████████████████████████████████| 20/20 [1:58:46<00:00, 356.32s/it]\n",
    "# [169, 501, 473, 520, 508]\n",
    "# [np.float64(0.5526849348267582), np.float64(0.5332538801811244), np.float64(0.5363925896434576), np.float64(0.536249048073956), np.float64(0.5469522986173644)]\n",
    "# 22088.729598522186\n",
    "# Took a very long time - several hours\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b66ba30-4464-4f8b-8807-3efbf1dda921",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "Using Cross fold validation did not pair well with the mango optimiser, this may be due to selecting hyperparameters that overfit for the training set, as the testing set is not used at all in that optimiser. As a result, it chose a very low min_samples_split value for the first tree, which resulted in a very average mse value overall for the Ensemble. When actually testing the set of hyper-parameters $[169, 501, 473, 520, 508]$ on our test data, we obtain an mse of $0.5237462918578$\n",
    "\n",
    "Our best performing optimiser was using Mango and finding the mss for each tree separately instead of all at once, which gave an mse of around $0.496$ with the array of mss being $[448, 464, 314, 374, 411]$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d5d601-f5ff-4628-b110-c57d492cb49d",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m min_samples \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m448\u001b[39m,\u001b[38;5;241m464\u001b[39m,\u001b[38;5;241m314\u001b[39m,\u001b[38;5;241m374\u001b[39m,\u001b[38;5;241m411\u001b[39m]\n\u001b[1;32m      2\u001b[0m rte \u001b[38;5;241m=\u001b[39m RegressionTreeEnsemble(\u001b[38;5;241m5\u001b[39m, min_samples)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mrte\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m rte_predictions \u001b[38;5;241m=\u001b[39m rte\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMSE produced by an Ensemble Regression Tree with best parameters found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmean_squared_error(y_test,\u001b[38;5;250m \u001b[39mrte_predictions)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[6], line 17\u001b[0m, in \u001b[0;36mRegressionTreeEnsemble.fit_tree\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     15\u001b[0m residuals \u001b[38;5;241m=\u001b[39m y \u001b[38;5;241m-\u001b[39m y_pred\n\u001b[1;32m     16\u001b[0m tree_n \u001b[38;5;241m=\u001b[39m RegressionTree(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_samples_split[i])\n\u001b[0;32m---> 17\u001b[0m \u001b[43mtree_n\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresiduals\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m y_pred \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(tree_n\u001b[38;5;241m.\u001b[39mpredict(X), index\u001b[38;5;241m=\u001b[39my\u001b[38;5;241m.\u001b[39mindex, columns\u001b[38;5;241m=\u001b[39my\u001b[38;5;241m.\u001b[39mcolumns)         \n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrees\u001b[38;5;241m.\u001b[39mappend(tree_n)\n",
      "Cell \u001b[0;32mIn[4], line 25\u001b[0m, in \u001b[0;36mRegressionTree.fit_tree\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     23\u001b[0m train \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([X,y], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# call recursive build function to create Regression Tree\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtree \u001b[38;5;241m=\u001b[39m \u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmin_samples_split\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 21\u001b[0m, in \u001b[0;36mbuild\u001b[0;34m(df, min_samples)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmean(df\u001b[38;5;241m.\u001b[39miloc[:,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# recursively build left and right child of current node\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# using the left and right split respectively\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m left_child \u001b[38;5;241m=\u001b[39m \u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mleft_split\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_samples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m right_child \u001b[38;5;241m=\u001b[39m build(df\u001b[38;5;241m.\u001b[39miloc[right_split], min_samples)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m TreeNode(mse, feature, value, left_child, right_child)\n",
      "Cell \u001b[0;32mIn[3], line 11\u001b[0m, in \u001b[0;36mbuild\u001b[0;34m(df, min_samples)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmean(df\u001b[38;5;241m.\u001b[39miloc[:,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# find the best mse split for the current dataset\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m mse, feature, value, left_split, right_split \u001b[38;5;241m=\u001b[39m \u001b[43mbest_mse_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# no split could be found, make it a leaf node\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# no non-empty split could be found -> remember if we had duplicate feature vectors for our entire dataset, no split could be found\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# as the mse is never updated, so we return the mean of ground truth values\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mse \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39minf:\n",
      "Cell \u001b[0;32mIn[2], line 31\u001b[0m, in \u001b[0;36mbest_mse_split\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):  \n\u001b[1;32m     27\u001b[0m     \u001b[38;5;66;03m# this is O(1), instead of recalculating the mse for the dataset, we can incrementally add an example to the left split and remove it from the right\u001b[39;00m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;66;03m# Since calculating mse with the traditional formula takes O(n), by sorting the current feature, we can incrementally control the size of each split\u001b[39;00m\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;66;03m# and calculate mse of each split in constant time\u001b[39;00m\n\u001b[1;32m     30\u001b[0m     left_split_y_sum \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39miloc[i]\n\u001b[0;32m---> 31\u001b[0m     left_split_y_squared_sum \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[1;32m     32\u001b[0m     right_split_y_sum \u001b[38;5;241m=\u001b[39m total_y_sum \u001b[38;5;241m-\u001b[39m left_split_y_sum\n\u001b[1;32m     33\u001b[0m     right_split_y_squared_sum \u001b[38;5;241m=\u001b[39m total_y_squared_sum \u001b[38;5;241m-\u001b[39m left_split_y_squared_sum\n",
      "File \u001b[0;32m/local/java/python-venv-packages.cs342/venv/lib/python3.9/site-packages/pandas/core/indexing.py:1191\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1189\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mapply_if_callable(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj)\n\u001b[1;32m   1190\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_deprecated_callable_usage(key, maybe_callable)\n\u001b[0;32m-> 1191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaybe_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/local/java/python-venv-packages.cs342/venv/lib/python3.9/site-packages/pandas/core/indexing.py:1722\u001b[0m, in \u001b[0;36m_iLocIndexer._getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1720\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mEllipsis\u001b[39m:\n\u001b[1;32m   1721\u001b[0m     key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mslice\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m-> 1722\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mABCDataFrame\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1723\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\n\u001b[1;32m   1724\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrame indexer is not allowed for .iloc\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1725\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConsider using .loc for automatic alignment.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1726\u001b[0m     )\n\u001b[1;32m   1728\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mslice\u001b[39m):\n",
      "File \u001b[0;32m/local/java/python-venv-packages.cs342/venv/lib/python3.9/site-packages/pandas/core/dtypes/generic.py:42\u001b[0m, in \u001b[0;36mcreate_pandas_abc_type.<locals>._instancecheck\u001b[0;34m(cls, inst)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(inst, attr, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_typ\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01min\u001b[39;00m comp\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# https://github.com/python/mypy/issues/1006\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# error: 'classmethod' used with a non-method\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_instancecheck\u001b[39m(\u001b[38;5;28mcls\u001b[39m, inst) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _check(inst) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inst, \u001b[38;5;28mtype\u001b[39m)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_subclasscheck\u001b[39m(\u001b[38;5;28mcls\u001b[39m, inst) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;66;03m# Raise instead of returning False\u001b[39;00m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;66;03m# This is consistent with default __subclasscheck__ behavior\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "min_samples = [448,464,314,374,411]\n",
    "rte = RegressionTreeEnsemble(5, min_samples)\n",
    "rte.fit_tree(X_train, y_train)\n",
    "rte_predictions = rte.predict(X_test)\n",
    "print(f\"MSE produced by an Ensemble Regression Tree with best parameters found: {mean_squared_error(y_test, rte_predictions)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cf9618-9b92-4703-a854-f80f97358368",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mean_squared_error' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m rte\u001b[38;5;241m.\u001b[39mfit_tree(X_train, y_train)\n\u001b[1;32m      4\u001b[0m rte_predictions \u001b[38;5;241m=\u001b[39m rte\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMSE produced by an Regression Tree with best parameter found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmean_squared_error(y_test,\u001b[38;5;250m \u001b[39mrte_predictions)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# we see the Ensemble Regression Tree outperforms the basic Regression Tree\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mean_squared_error' is not defined"
     ]
    }
   ],
   "source": [
    "# In comparison with a single tree\n",
    "rte = RegressionTree(448)\n",
    "rte.fit_tree(X_train, y_train)\n",
    "rte_predictions = rte.predict(X_test)\n",
    "print(f\"MSE produced by an Regression Tree with best parameter found: {mean_squared_error(y_test, rte_predictions)}\")\n",
    "\n",
    "# we see the Ensemble Regression Tree outperforms the basic Regression Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de78950-262d-46a4-8611-d5269e3bf062",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
